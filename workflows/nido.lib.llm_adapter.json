{
  "name": "nido.lib.llm_adapter",
  "nodes": [
    {
      "parameters": {},
      "name": "Execute Workflow Trigger",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [
        160,
        2928
      ],
      "id": "93102aa3-1263-42bd-bdb2-8653ad0a6d02"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Defaults and Environment Config\nconst json = $input.item.json;\nconst params = $input.params || {};\nconst fs = require('fs');\n\nconst prompt_id = json.prompt_id || params.prompt_id;\nconst input = json.input || params.input || {};\nconst options = json.options || params.options || {};\nconst force_refresh = json.force_refresh || params.force_refresh || false;\n// Extract chat_id from params if available, allowing persistent typing feedback\nconst chat_id = json.chat_id || params.chat_id || null;\n\nlet env_mode = 'live';\ntry {\n  env_mode = $env.LLM_MODE || 'live';\n} catch (e) {\n  // Access denied to env vars, defaulting to live\n}\n\n// Load models from config if available\nlet models = options.model;\n\nif (prompt_id) {\n    try {\n        const modelsConfigPath = '/data/infra/models.json';\n        // Try strict path first (docker mapping), fallback to relative if dev env\n        let configPath = modelsConfigPath;\n        if (!fs.existsSync(configPath)) {\n             // Fallback for local dev environment\n             configPath = '/Users/alberto/src/nido/infra/synology/models.json';\n        }\n        \n        if (fs.existsSync(configPath)) {\n            const modelsConfig = JSON.parse(fs.readFileSync(configPath, 'utf8'));\n            if (modelsConfig[prompt_id]) {\n                models = modelsConfig[prompt_id];\n            }\n        }\n    } catch (e) {\n        // Config load failed, keep existing models\n        console.log('Failed to load models config: ' + e.message);\n    }\n}\n\n// Fallback default if still no models\nif (!models) {\n    models = ['models/gemini-3-flash-preview'];\n}\n\n// Normalize model to array for fallback support\nif (!Array.isArray(models)) {\n  models = [models];\n}\n\n// Validation\nif (!prompt_id) {\n  throw new Error(`Missing parameter: prompt_id`);\n}\nif (!input.input_type || !['TEXT', 'IMAGE', 'AUDIO'].includes(input.input_type)) {\n    throw new Error('Missing or invalid parameter: input.input_type (must be TEXT, IMAGE, or AUDIO)');\n}\nif (input.input_type === 'TEXT' && !input.input_text) {\n     throw new Error('Missing parameter: input.input_text for TEXT input');\n}\nif ((input.input_type === 'IMAGE' || input.input_type === 'AUDIO') && !input.input_path) {\n     throw new Error(`Missing parameter: input.input_path for ${input.input_type} input`);\n}\n\nreturn {\n  json: {\n    prompt_id,\n    input,\n    chat_id,\n    llm_options: { ...options, model: models },\n    force_refresh,\n    env_mode,\n    models,\n    model_index: 0\n  }\n};"
      },
      "name": "Init Config",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        384,
        2544
      ],
      "id": "0075885b-1e82-4d61-828c-687cf7b60f62"
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{ !!$json.chat_id && $json.chat_id !== 'nido-web' }}",
              "value2": true
            }
          ]
        }
      },
      "name": "Has Chat ID?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        608,
        2336
      ],
      "id": "1ee831ee-8a16-4596-9a69-b4511cc19d74"
    },
    {
      "parameters": {
        "operation": "sendChatAction",
        "chatId": "={{ $json.chat_id }}"
      },
      "name": "Typing",
      "type": "n8n-nodes-base.telegram",
      "typeVersion": 1.2,
      "position": [
        832,
        2336
      ],
      "id": "575ff1bd-ff20-417c-a461-85554116d8ea",
      "webhookId": "6f9894f6-f10f-424e-b3e7-410275619e6e",
      "credentials": {
        "telegramApi": {
          "id": "JLG33gMQcdi4JvQq",
          "name": "Telegram account"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{ $json.cache_hit }}",
              "value2": true
            }
          ]
        }
      },
      "name": "Cache Hit?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        1504,
        2816
      ],
      "id": "ff081e37-92bc-4470-8099-f2d1a0197b57"
    },
    {
      "parameters": {
        "dataType": "string",
        "value1": "={{ $json.env_mode }}",
        "rules": {
          "rules": [
            {
              "value2": "live"
            },
            {
              "value2": "mock",
              "output": 1
            }
          ]
        },
        "fallbackOutput": 2
      },
      "name": "Handle Miss",
      "type": "n8n-nodes-base.switch",
      "typeVersion": 1,
      "position": [
        1728,
        2688
      ],
      "id": "fdd7048c-4153-4bcc-9057-8816824b414f"
    },
    {
      "parameters": {
        "errorMessage": "Error al acceder a la cache"
      },
      "name": "Error: Cache Missing",
      "type": "n8n-nodes-base.stopAndError",
      "typeVersion": 1,
      "position": [
        1952,
        2992
      ],
      "id": "a554c5ac-6ca4-4c6b-b669-806408880a98"
    },
    {
      "parameters": {
        "jsCode": "return { json: { output: 'MOCK_DATA_PLACEHOLDER', source: 'mock' } }; // TODO: Implement robust mock generation from schema"
      },
      "name": "Generate Mock",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        4544,
        3296
      ],
      "id": "cd932fc7-aa87-4bd5-a572-0c4d27f59a3f"
    },
    {
      "parameters": {
        "operation": "write",
        "fileName": "={{ $json.cache_path }}",
        "options": {
          "append": false
        }
      },
      "name": "Save Cache",
      "type": "n8n-nodes-base.readWriteFile",
      "typeVersion": 1,
      "position": [
        4544,
        2752
      ],
      "id": "750c7d5d-5886-4c29-a88e-6eda08d2c3c3"
    },
    {
      "parameters": {
        "jsCode": "const result = $input.first().json;\nlet output = result.output || result; // Handle structure from different paths\nlet source = result.source || 'cache'; // Default to cache if not specified\n\n// Get input config for _meta\nconst initConfig = $('Init Config').first().json;\n\n// Check for OpenAI response format (choices array OR flat message)\nif (result.choices && result.choices[0] && result.choices[0].message) {\n    const rawText = result.choices[0].message.content || '';\n    source = result.source || 'live-openai';\n    try {\n        const cleanText = rawText.replace(/```json\\n?|\\n?```/g, '').trim();\n        output = JSON.parse(cleanText);\n    } catch (e) {\n        output = { raw_text: rawText, error: 'JSON Parse Error' };\n    }\n} else if (result.message && result.message.content) {\n    const rawText = result.message.content || '';\n    source = result.source || 'live-openai';\n    try {\n        const cleanText = rawText.replace(/```json\\n?|\\n?```/g, '').trim();\n        output = JSON.parse(cleanText);\n    } catch (e) {\n        output = { raw_text: rawText, error: 'JSON Parse Error' };\n    }\n}\n// Check for Gemini response format (content.parts)\nelse if (result.content && result.content.parts && result.content.parts[0]) {\n    const rawText = result.content.parts[0].text || '';\n    source = result.source || 'live-gemini';\n    try {\n        const cleanText = rawText.replace(/```json\\n?|\\n?```/g, '').trim();\n        output = JSON.parse(cleanText);\n    } catch (e) {\n        output = { raw_text: rawText, error: 'JSON Parse Error' };\n    }\n} else if (typeof result.content === 'string') {\n    const rawText = result.content || '';\n    source = result.source || 'live-openai';\n    try {\n        const cleanText = rawText.replace(/```json\\n?|\\n?```/g, '').trim();\n        output = JSON.parse(cleanText);\n    } catch (e) {\n        output = { raw_text: rawText, error: 'JSON Parse Error' };\n    }\n} else if (!output.output && result.text) {\n    // Old format fallback\n    try {\n        const cleanText = result.text.replace(/```json\\n?|\\n?```/g, '').trim();\n        output = JSON.parse(cleanText);\n    } catch (e) {\n        output = { raw_text: result.text, error: 'JSON Parse Error' };\n    }\n}\n\nreturn {\n  json: {\n    ...output,\n    _meta: {\n      prompt_id: initConfig.prompt_id,\n      input_type: initConfig.input.input_type,\n      input_path: initConfig.input.input_path || null,\n      input_text: initConfig.input.input_text || null,\n      source: source\n    }\n  }\n};\n"
      },
      "name": "Format Output",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        4768,
        3200
      ],
      "id": "3c6174e4-1a9f-48e4-8c4a-19510068eddf"
    },
    {
      "parameters": {
        "jsCode": "const items = $input.all();\nconsole.log('--- ADAPTER FINAL OUTPUT ---');\nconsole.log(JSON.stringify(items));\nif (items.length === 0) {\n    console.log('WARNING: Adapter returning 0 items!');\n}\nreturn items;"
      },
      "name": "Debug Final Output",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        4992,
        3200
      ],
      "id": "a40b3d1e-54b2-420e-b003-254ec2c4ba0a"
    },
    {
      "parameters": {
        "jsCode": "if ($input.first().json.input.input_type !== 'TEXT') {\n  return [\n    {\n      json: {\n        hash: '',\n      },\n    },\n  ];\n}\n\nconst crypto = require('crypto');\n\n// Access upstream data securely\n// We access the Merge Files output, or directly from the source nodes if accessible in scope, \n// but assuming Merge Files passes them through. \n// Better yet, since Merge Files output is tricky ($input.all() vs items), let's use $items('Node Name')\nconst input_text = $('Execute Workflow Trigger').first().json.input.input_text;\n\n// Calculate Hashes\nconst input_text_hash = crypto.createHash('sha256').update(input_text).digest('hex');\n\nreturn {\n  json: {\n    hash: input_text_hash\n  }\n};"
      },
      "name": "Hash Input Text",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        608,
        3120
      ],
      "id": "935b540b-d495-4857-a321-edc91ac94c49"
    },
    {
      "parameters": {
        "jsCode": "if ($input.first().json.input.input_type === 'TEXT') {\n  return [\n    {\n      json: {\n        hash: '',\n      },\n    },\n  ];\n}\n\nconst crypto = require('crypto');\nconst fs = require('fs');\n\n// Access upstream data securely\n// We access the Merge Files output, or directly from the source nodes if accessible in scope, \n// but assuming Merge Files passes them through. \n// Better yet, since Merge Files output is tricky ($input.all() vs items), let's use $items('Node Name')\nvar input_hash_data = '';\ntry {\n  const input_data = fs.readFileSync($input.first().json.input.input_path);\n  input_hash_data = crypto.createHash('sha256').update(input_data).digest('hex');\n} catch (e) {\n    throw new Error(`Failed to read input file at ${$input.first().json.input.input_path}: ${e.message}`);\n}\n\nreturn {\n  json: {\n    hash: input_hash_data\n  }\n};"
      },
      "name": "Hash Input File",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        608,
        2928
      ],
      "id": "cb2977f4-df51-4ad2-ad14-1e0f34f60cba"
    },
    {
      "parameters": {
        "numberInputs": 4
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2,
      "position": [
        832,
        2784
      ],
      "id": "0c8547c5-e40a-4f79-9d40-5f87de9998be",
      "name": "Merge Input Hash"
    },
    {
      "parameters": {
        "jsCode": "const crypto = require('crypto');\nconst fs = require('fs');\n\nconst prompt_id = $input.first().json.prompt_id;\nlet basePath = `/prompts/${prompt_id}`;\ntry {\n    if (fs.existsSync('/prompts/registry.json')) {\n        const registry = JSON.parse(fs.readFileSync('/prompts/registry.json', 'utf8'));\n        if (registry[prompt_id] && registry[prompt_id].path) {\n             basePath = `/prompts/${registry[prompt_id].path}`;\n        }\n    }\n} catch (e) {}\n\nconst path = `${basePath}/prompt.md`;\nvar input_hash_data = '';\ntry {\n  const input_data = fs.readFileSync(path);\n  input_hash_data = crypto.createHash('sha256').update(input_data).digest('hex');\n} catch (e) {\n    throw new Error(`Failed to read input file at ${path}: ${e.message}`);\n}\n\nreturn {\n  json: {\n    hash: input_hash_data\n  }\n};"
      },
      "name": "Hash Prompt",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        608,
        2544
      ],
      "id": "1e04cd99-9f62-4ad1-99e3-3f813d765be5"
    },
    {
      "parameters": {
        "jsCode": "const combinedHash = $input.all()\n  .map(item => item.json.hash)\n  .join('');\n\n// Restore context from Init Config\nconst context = $('Init Config').first().json;\n\nreturn [\n  {\n    json: {\n      ...context,\n      hash: combinedHash,\n      cache_key: combinedHash\n    },\n  },\n];"
      },
      "name": "Hash Input",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1056,
        2816
      ],
      "id": "3852cad1-6699-4677-b769-b7adc8366921"
    },
    {
      "parameters": {
        "jsCode": "const crypto = require('crypto');\nconst fs = require('fs');\n\nconst prompt_id = $input.first().json.prompt_id;\nlet basePath = `/prompts/${prompt_id}`;\ntry {\n    if (fs.existsSync('/prompts/registry.json')) {\n        const registry = JSON.parse(fs.readFileSync('/prompts/registry.json', 'utf8'));\n        if (registry[prompt_id] && registry[prompt_id].path) {\n             basePath = `/prompts/${registry[prompt_id].path}`;\n        }\n    }\n} catch (e) {}\n\nconst path = `${basePath}/schema.json`;\nvar input_hash_data = '';\ntry {\n  const input_data = fs.readFileSync(path);\n  input_hash_data = crypto.createHash('sha256').update(input_data).digest('hex');\n} catch (e) {\n    throw new Error(`Failed to read input file at ${path}: ${e.message}`);\n}\n\nreturn {\n  json: {\n    hash: input_hash_data\n  }\n};\n"
      },
      "name": "Hash Schema",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        608,
        2736
      ],
      "id": "bbb658d6-66f6-4f41-a6fa-5e54fa3d9354"
    },
    {
      "parameters": {
        "rules": {
          "values": [
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                },
                "conditions": [
                  {
                    "leftValue": "={{ $json.input.input_type }}",
                    "rightValue": "TEXT",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    },
                    "id": "ca02d6e8-5b20-4147-a56e-e1281ec42a87"
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "text"
            },
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                },
                "conditions": [
                  {
                    "id": "75a89306-db00-4409-b350-1d3bc1703b69",
                    "leftValue": "={{ $json.input.input_type }}",
                    "rightValue": "IMAGE",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    }
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "image"
            },
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                },
                "conditions": [
                  {
                    "id": "5a575b14-eaf7-4e6e-8f55-ff8f8b417ceb",
                    "leftValue": "={{ $json.input.input_type }}",
                    "rightValue": "AUDIO",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    }
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "audio"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3.4,
      "position": [
        2400,
        2512
      ],
      "id": "4cf92d89-f641-458a-99e1-b897c126983f",
      "name": "Modality"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "={{ $json.current_model }}",
          "mode": "id"
        },
        "messages": {
          "values": [
            {
              "content": "=Prompt:\n{{ $json.prompt_content }}\n\nSchema:\n{{ $json.schema_content }}"
            },
            {
              "content": "=Input:\n{{ $json.input.input_text }}"
            }
          ]
        },
        "builtInTools": {},
        "options": {}
      },
      "name": "Call Google Gemini Text",
      "type": "@n8n/n8n-nodes-langchain.googleGemini",
      "typeVersion": 1.1,
      "position": [
        3296,
        2000
      ],
      "id": "d2d0a6fc-5acd-4508-8a1c-a407894e19ed",
      "credentials": {
        "googlePalmApi": {
          "id": "JJHsBmH2qC9U0KQA",
          "name": "Google Gemini(PaLM) Api account"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "resource": "image",
        "operation": "analyze",
        "modelId": {
          "__rl": true,
          "value": "={{ $('Prepare Model Call').first().json.current_model }}",
          "mode": "id"
        },
        "text": "={{ $('Prepare Model Call').first().json.prompt_content }}\n\nSchema:\n{{ $('Prepare Model Call').first().json.schema_content }}",
        "inputType": "binary",
        "options": {}
      },
      "name": "Call Google Gemini Image",
      "type": "@n8n/n8n-nodes-langchain.googleGemini",
      "typeVersion": 1.1,
      "position": [
        3360,
        2384
      ],
      "id": "6e52416e-2385-4cac-972f-b0acb8c91e82",
      "credentials": {
        "googlePalmApi": {
          "id": "JJHsBmH2qC9U0KQA",
          "name": "Google Gemini(PaLM) Api account"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "fileSelector": "={{ $json.input.input_path }}",
        "options": {}
      },
      "type": "n8n-nodes-base.readWriteFile",
      "typeVersion": 1.1,
      "position": [
        2624,
        2480
      ],
      "id": "f1b5ede5-de3e-498c-a1b6-18a691098de8",
      "name": "Modality Get Image"
    },
    {
      "parameters": {
        "fileSelector": "={{ $json.input.input_path }}",
        "options": {}
      },
      "type": "n8n-nodes-base.readWriteFile",
      "typeVersion": 1.1,
      "position": [
        3072,
        2768
      ],
      "id": "89fb5b18-205e-4f1c-805c-45ce338bccb5",
      "name": "Modality Get Audio"
    },
    {
      "parameters": {
        "resource": "audio",
        "operation": "analyze",
        "modelId": {
          "__rl": true,
          "value": "={{ $('Modality').item.json.llm_options.model }}",
          "mode": "id"
        },
        "text": "={{ $('Modality').item.json.prompt_content }}\n\nSchema:\n{{ $('Modality').item.json.schema_content }}",
        "inputType": "binary",
        "binaryPropertyName": "image",
        "options": {}
      },
      "name": "Call Google Gemini Audio",
      "type": "@n8n/n8n-nodes-langchain.googleGemini",
      "typeVersion": 1.1,
      "position": [
        3360,
        2768
      ],
      "id": "6bdcf579-974c-40be-b182-981c861ef273",
      "credentials": {
        "googlePalmApi": {
          "id": "JJHsBmH2qC9U0KQA",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "\nconst fs = require('fs');\n\n// Get prompt_id from Init Config\nconst prompt_id = $('Init Config').first().json.prompt_id;\n\n// Resolve Path via Registry\nlet basePath = `/prompts/${prompt_id}`;\ntry {\n    if (fs.existsSync('/prompts/registry.json')) {\n        const registry = JSON.parse(fs.readFileSync('/prompts/registry.json', 'utf8'));\n        if (registry[prompt_id] && registry[prompt_id].path) {\n             basePath = `/prompts/${registry[prompt_id].path}`;\n        }\n    }\n} catch (e) {\n    // Ignore registry errors, fallback to default path\n    console.log('Registry lookup failed: ' + e.message);\n}\n\n// Construct paths\nconst promptPath = `${basePath}/prompt.md`;\nconst schemaPath = `${basePath}/schema.json`;\n\n// Read files\nlet prompt_content = '';\nlet schema_content = '';\n\ntry {\n    prompt_content = fs.readFileSync(promptPath, 'utf8');\n} catch (e) {\n    throw new Error(`Failed to read prompt file at ${promptPath}: ${e.message}`);\n}\n\ntry {\n    schema_content = fs.readFileSync(schemaPath, 'utf8');\n} catch (e) {\n    throw new Error(`Failed to read schema file at ${schemaPath}: ${e.message}`);\n}\n\n// Pass through original input and add context\nconst input = $input.first().json;\n\n// Calculate today\nconst now = new Date();\nconst today = now.toISOString().split('T')[0];\n\nif (prompt_content) {\n    prompt_content = prompt_content.replace(/{{current_date}}/g, today);\n}\n\nreturn {\n    json: {\n        ...input,\n        prompt_content,\n        schema_content\n    }\n};\n"
      },
      "name": "Load Context",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1952,
        2624
      ],
      "id": "135293fd-0226-4fb0-b78c-5eaf86086e1e"
    },
    {
      "parameters": {
        "jsCode": "\nconst fs = require('fs');\n\n// Convert LLM JSON response to binary for file storage\nconst response = $input.first().json;\n\n// Extract the text content (which contains JSON wrapped in markdown)\nlet textContent = '';\n\n// Check for OpenAI response format\nif (response.choices && response.choices[0] && response.choices[0].message) {\n    textContent = response.choices[0].message.content || '';\n} else if (response.message && response.message.content) {\n    textContent = response.message.content || '';\n}\n// Check for Gemini response format\nelse if (response.content && response.content.parts && response.content.parts[0]) {\n    textContent = response.content.parts[0].text || '';\n}\n// Check for simple content string (OpenAI Image)\nelse if (typeof response.content === 'string') {\n    textContent = response.content;\n}\n\n// Clean markdown wrapping if present\nconst cleanJson = textContent.replace(/```json\\n?|\\n?```/g, '').trim();\n\n// Get cache path and extract directory using string manipulation (no path module)\nconst cachePath = $('Check Cache').first().json.cache_path;\nconst cacheDir = cachePath.substring(0, cachePath.lastIndexOf('/'));\n\n// Create directory if it doesn't exist\nif (!fs.existsSync(cacheDir)) {\n    fs.mkdirSync(cacheDir, { recursive: true });\n}\n\n// Determine source based on provider\nconst prepareData = $('Prepare Model Call').first().json;\nconst source = prepareData.provider === 'openai' ? 'live-openai' : 'live-gemini';\n\n// Convert to binary buffer\nconst buffer = Buffer.from(cleanJson, 'utf8');\n\nreturn {\n    json: {\n        ...response,\n        cache_path: cachePath,\n        source: source\n    },\n    binary: {\n        data: {\n            data: buffer.toString('base64'),\n            mimeType: 'application/json',\n            fileName: 'cache.json'\n        }\n    }\n};\n"
      },
      "name": "Prepare Cache",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        4320,
        2752
      ],
      "id": "670f5bda-f955-4761-981b-e97458e69990"
    },
    {
      "parameters": {
        "jsCode": "const fs = require('fs');\nconst path = `/data/llm_cache/${items[0].json.prompt_id}/${items[0].json.cache_key}.json`;\n\nconst mode = items[0].json.env_mode;\nconst force = items[0].json.force_refresh;\n\nif (mode !== 'live' || !force) {\n  if (fs.existsSync(path)) {\n      try {\n          const cached = JSON.parse(fs.readFileSync(path, 'utf8'));\n          return [{ json: { ...items[0].json, output: cached, cache_hit: true, cache_path: path } }];\n      } catch (e) {\n          // Corrupted cache file, treat as miss\n      }\n  }\n}\n\nreturn [{ json: { ...items[0].json, output: null, cache_hit: false, cache_path: path } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1280,
        2816
      ],
      "id": "0eb746c7-5995-4cf5-9dd9-d824e692ae8b",
      "name": "Check Cache"
    },
    {
      "parameters": {
        "jsCode": "// Prepare model call - determine current model and provider\nconst input = $input.first().json;\nconst models = input.models || ['models/gemini-3-flash-preview'];\nconst idx = input.model_index || 0;\nlet currentModel = models[idx];\n\n// Determine provider from model name\nconst isOpenAI = currentModel.includes('gpt') || currentModel.includes('openai');\nconst provider = isOpenAI ? 'openai' : 'gemini';\n\n// OpenAI API doesn't accept 'models/' prefix, strip it if present\nif (isOpenAI && currentModel.startsWith('models/')) {\n  currentModel = currentModel.replace('models/', '');\n}\n\nreturn {\n  json: {\n    ...input,\n    current_model: currentModel,\n    provider,\n    has_more_models: idx < models.length - 1\n  }\n};"
      },
      "name": "Prepare Model Call",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2176,
        2624
      ],
      "id": "3cf9c46b-b468-4fc4-9ba4-d19ae09217db"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "={{ $json.current_model }}",
          "mode": "id"
        },
        "messages": {
          "values": [
            {
              "content": "=Prompt:\n{{ $json.prompt_content }}\n\nSchema:\n{{ $json.schema_content }}"
            },
            {
              "content": "=Input:\n{{ $json.input.input_text }}"
            }
          ]
        },
        "options": {
          "temperature": 0.7
        }
      },
      "name": "Call OpenAI Text",
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        3360,
        2192
      ],
      "id": "03f465d3-576f-43cb-97e6-5793ebe93056",
      "credentials": {
        "openAiApi": {
          "id": "5FPvnzDIF3vm0DYw",
          "name": "OpenAi account"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "resource": "image",
        "operation": "analyze",
        "modelId": {
          "__rl": true,
          "value": "={{ $('Prepare Model Call').first().json.current_model }}",
          "mode": "id"
        },
        "text": "={{ $('Prepare Model Call').first().json.prompt_content }}\n\nSchema:\n{{ $('Prepare Model Call').first().json.schema_content }}",
        "inputType": "base64",
        "options": {}
      },
      "name": "Call OpenAI Image",
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        3360,
        2576
      ],
      "id": "191cb7eb-dc06-4275-9930-3c9919ddc904",
      "credentials": {
        "openAiApi": {
          "id": "5FPvnzDIF3vm0DYw",
          "name": "OpenAi account"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "// Handle LLM response - check for errors and retry with fallback if needed\nconst response = $input.first().json;\nconst prepareData = $('Prepare Model Call').first().json;\n\n// --- Response Parsing ---\nlet output = null;\nlet parsingError = null;\n\n// Normalize content extraction (borrowed from Format Output logic)\nlet rawText = '';\nif (response.choices && response.choices[0] && response.choices[0].message) {\n    rawText = response.choices[0].message.content || '';\n} else if (response.message && response.message.content) {\n    rawText = response.message.content || '';\n} else if (response.content && response.content.parts && response.content.parts[0]) {\n    rawText = response.content.parts[0].text || '';\n} else if (typeof response.content === 'string') {\n    rawText = response.content;\n}\n\ntry {\n    const cleanText = rawText.replace(/```json\\n?|\\n?```/g, '').trim();\n    if (cleanText) {\n        output = JSON.parse(cleanText);\n    }\n} catch (e) {\n    parsingError = 'JSON Parse Error';\n}\n\n// --- Schema Validation ---\nlet validationError = null;\nif (!parsingError && output) {\n    try {\n        const schema = JSON.parse(prepareData.schema_content || '{}');\n        if (schema.required && Array.isArray(schema.required)) {\n            const missing = schema.required.filter(field => !Object.prototype.hasOwnProperty.call(output, field));\n            if (missing.length > 0) {\n                validationError = `Missing required fields: ${missing.join(', ')}`;\n            }\n        }\n    } catch (e) {\n        // Iterate gently, if schema is invalid don't block but maybe warn\n        console.log('Schema validation skipped due to schema parse error');\n    }\n}\n\n// --- Error Detection ---\n// Check if this looks like an error response OR validation failed\nconst errMsg = response.error || response.errorMessage || (typeof response.message === 'string' ? response.message : '');\n\nconst hasError = !!errMsg || \n                (errMsg && errMsg.includes('error')) || \n                (errMsg && errMsg.includes('Cannot use \\'in\\' operator')) || // Specific n8n node error\n                (response.output && response.output.error) ||\n                !!parsingError ||\n                !!validationError;\n\nif (hasError && prepareData.has_more_models) {\n  const reason = validationError || parsingError || errMsg || 'Unknown error';\n  // Return signal to retry with next model\n  return {\n    json: {\n      ...prepareData,\n      model_index: prepareData.model_index + 1,\n      retry_needed: true,\n      previous_error: reason\n    }\n  };\n}\n\n// Success - pass through the response\nreturn {\n  json: {\n    ...response,\n    source: prepareData.provider === 'openai' ? 'live-openai' : 'live-gemini'\n  }\n};"
      },
      "name": "Handle LLM Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3648,
        2416
      ],
      "id": "a41372bd-b994-496d-a663-91ac16184196"
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{ $json.retry_needed }}",
              "value2": true
            }
          ]
        }
      },
      "name": "Retry Needed?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        4096,
        2752
      ],
      "id": "eddf7e47-ae09-4eb3-8e95-dcf5f3f1cd25"
    },
    {
      "parameters": {
        "rules": {
          "values": [
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                },
                "conditions": [
                  {
                    "leftValue": "={{ $json.provider }}",
                    "rightValue": "gemini",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    },
                    "id": "provider-gemini"
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "gemini"
            },
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                },
                "conditions": [
                  {
                    "id": "provider-openai",
                    "leftValue": "={{ $json.provider }}",
                    "rightValue": "openai",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    }
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "openai"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3.4,
      "position": [
        3072,
        2096
      ],
      "id": "931166a7-ed4d-4337-a0ad-6e0033caeda4",
      "name": "Provider Router"
    },
    {
      "parameters": {
        "rules": {
          "values": [
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                },
                "conditions": [
                  {
                    "leftValue": "={{ $('Prepare Model Call').first().json.provider }}",
                    "rightValue": "gemini",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    },
                    "id": "provider-gemini-img"
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "gemini"
            },
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 3
                },
                "conditions": [
                  {
                    "id": "provider-openai-img",
                    "leftValue": "={{ $('Prepare Model Call').first().json.provider }}",
                    "rightValue": "openai",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    }
                  }
                ],
                "combinator": "and"
              },
              "renameOutput": true,
              "outputKey": "openai"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3.4,
      "position": [
        3072,
        2480
      ],
      "id": "959d4403-7000-4f15-aad9-4a752d853671",
      "name": "Provider Router Image"
    },
    {
      "parameters": {
        "jsCode": "// Combine JSON from Prepare Model Call with binary from input\nconst modelInfo = $('Prepare Model Call').first().json;\nconst binaryData = $input.first().binary;\n\nreturn {\n  json: modelInfo,\n  binary: binaryData\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2848,
        2480
      ],
      "id": "64875928-48f9-4961-9a56-75ac6167e78a",
      "name": "Merge Model Info Image"
    },
    {
      "parameters": {
        "jsCode": "// Log usage to a monthly CSV file\nconst fs = require('fs');\nconst inputItem = $input.first().json;\n\n// Determine date for filename\nconst now = new Date();\nconst year = now.getFullYear();\nconst month = String(now.getMonth() + 1).padStart(2, '0');\nconst logDir = '/data/logs';\nconst logFile = `${logDir}/llm_usage_${year}-${month}.csv`;\n\n// Ensure directory exists\nif (!fs.existsSync(logDir)) {\n    fs.mkdirSync(logDir, { recursive: true });\n}\n\n// Headers: timestamp,prompt_id,model,provider,input_type,status,tokens_in,tokens_out,total_tokens\nif (!fs.existsSync(logFile)) {\n    fs.writeFileSync(logFile, 'timestamp,prompt_id,model,provider,input_type,status,tokens_in,tokens_out,total_tokens\\n');\n}\n\n// Extract Data\nconst timestamp = now.toISOString();\nconst prepareData = $('Prepare Model Call').first().json;\nconst prompt_id = prepareData.prompt_id || 'unknown';\nconst model = prepareData.current_model || 'unknown';\nconst provider = prepareData.provider || 'unknown';\nconst input_type = prepareData.input && prepareData.input.input_type ? prepareData.input.input_type : 'unknown';\n\n// Status & Tokens\nlet status = 'SUCCESS';\nlet tokens_in = '';\nlet tokens_out = '';\nlet total_tokens = '';\n\nif (inputItem.retry_needed || inputItem.error || inputItem.errorMessage) {\n    status = 'ERROR';\n}\n\nif (inputItem.usage) {\n    tokens_in = inputItem.usage.prompt_tokens || '';\n    tokens_out = inputItem.usage.completion_tokens || '';\n    total_tokens = inputItem.usage.total_tokens || '';\n}\n\n// CSV Line\nconst line = `${timestamp},${prompt_id},${model},${provider},${input_type},${status},${tokens_in},${tokens_out},${total_tokens}\\n`;\n\ntry {\n    fs.appendFileSync(logFile, line);\n} catch(e) {\n    console.log('Error writing to log: ' + e.message);\n}\n\n// Pass through unmodified for Retry Logic\nreturn $input.all();"
      },
      "name": "Log Usage",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3872,
        2416
      ],
      "id": "30e74fdb-188a-4d6d-ace6-19cea5202b0a"
    }
  ],
  "pinData": {},
  "connections": {
    "Execute Workflow Trigger": {
      "main": [
        [
          {
            "node": "Init Config",
            "type": "main",
            "index": 0
          },
          {
            "node": "Hash Input File",
            "type": "main",
            "index": 0
          },
          {
            "node": "Hash Input Text",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Init Config": {
      "main": [
        [
          {
            "node": "Hash Prompt",
            "type": "main",
            "index": 0
          },
          {
            "node": "Hash Schema",
            "type": "main",
            "index": 0
          },
          {
            "node": "Has Chat ID?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Chat ID?": {
      "main": [
        [
          {
            "node": "Typing",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Cache Hit?": {
      "main": [
        [
          {
            "node": "Format Output",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Handle Miss",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Output": {
      "main": [
        [
          {
            "node": "Debug Final Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Mock": {
      "main": [
        [
          {
            "node": "Format Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save Cache": {
      "main": [
        [
          {
            "node": "Format Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Hash Input Text": {
      "main": [
        [
          {
            "node": "Merge Input Hash",
            "type": "main",
            "index": 3
          }
        ]
      ]
    },
    "Hash Input File": {
      "main": [
        [
          {
            "node": "Merge Input Hash",
            "type": "main",
            "index": 2
          }
        ]
      ]
    },
    "Merge Input Hash": {
      "main": [
        [
          {
            "node": "Hash Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Hash Input": {
      "main": [
        [
          {
            "node": "Check Cache",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Modality": {
      "main": [
        [
          {
            "node": "Provider Router",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Modality Get Image",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Modality Get Audio",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Google Gemini Text": {
      "main": [
        [
          {
            "node": "Handle LLM Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Modality Get Image": {
      "main": [
        [
          {
            "node": "Merge Model Info Image",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Provider Router Image": {
      "main": [
        [
          {
            "node": "Call Google Gemini Image",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Call OpenAI Image",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Modality Get Audio": {
      "main": [
        [
          {
            "node": "Call Google Gemini Audio",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Google Gemini Image": {
      "main": [
        [
          {
            "node": "Handle LLM Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Google Gemini Audio": {
      "main": [
        [
          {
            "node": "Handle LLM Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Handle Miss": {
      "main": [
        [
          {
            "node": "Load Context",
            "type": "main",
            "index": 0
          }
        ],
        [],
        [
          {
            "node": "Error: Cache Missing",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Load Context": {
      "main": [
        [
          {
            "node": "Prepare Model Call",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Hash Schema": {
      "main": [
        [
          {
            "node": "Merge Input Hash",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Hash Prompt": {
      "main": [
        [
          {
            "node": "Merge Input Hash",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Cache": {
      "main": [
        [
          {
            "node": "Save Cache",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check Cache": {
      "main": [
        [
          {
            "node": "Cache Hit?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Model Call": {
      "main": [
        [
          {
            "node": "Modality",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Provider Router": {
      "main": [
        [
          {
            "node": "Call Google Gemini Text",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Call OpenAI Text",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call OpenAI Text": {
      "main": [
        [
          {
            "node": "Handle LLM Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call OpenAI Image": {
      "main": [
        [
          {
            "node": "Handle LLM Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Handle LLM Response": {
      "main": [
        [
          {
            "node": "Log Usage",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Usage": {
      "main": [
        [
          {
            "node": "Retry Needed?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Retry Needed?": {
      "main": [
        [
          {
            "node": "Prepare Model Call",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Prepare Cache",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Model Info Image": {
      "main": [
        [
          {
            "node": "Provider Router Image",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "availableInMCP": false
  },
  "versionId": "6db905c7-a572-4a95-bb04-c9c488183311",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "e1f8529dafc1c7b24188afd104b1a8e8824324405683593c80fc8bc56ba17881"
  },
  "id": "cIdjAXxgI1Z-LurEg16oO",
  "tags": []
}