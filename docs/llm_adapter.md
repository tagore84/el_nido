Plan: Capa LLM desacoplada para desarrollo eficiente en n8n

Objetivo

Permitir el desarrollo, depuraci√≥n e iteraci√≥n de workflows que usan LLM sin realizar llamadas reales a modelos en cada ejecuci√≥n, manteniendo:
	‚Ä¢	Escalabilidad
	‚Ä¢	Bajo coste
	‚Ä¢	Reproducibilidad
	‚Ä¢	Control por entorno (dev / prod)

La soluci√≥n debe ser centralizada, versionable en Git y transparente para el resto de workflows.

‚∏ª

Principio clave

Ning√∫n workflow llama directamente a un proveedor LLM.

Todas las llamadas pasan por un sub-workflow est√°ndar: llm.call (LLM Adapter).

Esto permite cambiar el comportamiento global (live, replay, mock, cache) sin tocar los workflows funcionales.

‚∏ª

Arquitectura general

[Workflow funcional]
        ‚îÇ
        ‚ñº
[Sub-workflow: llm.call]
        ‚îÇ
        ‚îú‚îÄ IF LLM_MODE
        ‚îÇ    ‚îú‚îÄ live        ‚Üí LLM real + grabar cache
        ‚îÇ    ‚îú‚îÄ replay      ‚Üí cache / fixtures
        ‚îÇ    ‚îú‚îÄ mock        ‚Üí mock determinista
        ‚îÇ    ‚îî‚îÄ cache_only  ‚Üí error si no hay cache
        ‚îÇ
        ‚ñº
     output + meta


‚∏ª

Modos de ejecuci√≥n (feature flags)

Controlados mediante variables de entorno (docker-compose / Synology / n8n):

LLM_MODE=live | replay | mock | cache_only

Significado de cada modo

Modo	Uso principal	Comportamiento
live	Producci√≥n	Llama al modelo y guarda cache
replay	Desarrollo	Devuelve respuestas grabadas
mock	Desarrollo	Respuestas falsas pero v√°lidas
cache_only	Tests	Falla si no existe cache


‚∏ª

Contrato del sub-workflow llm.call

Inputs

{
  "prompt_id": "classify_image_v3",
  "prompt_version": "v3",
  "inputs": { "text": "..." },
  "options": {
    "model": "gpt-4.1-mini",
    "temperature": 0.2
  },
  "force_refresh": false
}

Outputs

{
  "output": { ... },
  "meta": {
    "prompt_id": "classify_image_v3",
    "prompt_version": "v3",
    "provider": "openai",
    "model": "gpt-4.1-mini",
    "cache_hit": true,
    "hash": "sha256...",
    "timestamp": "2026-01-12T12:00:00Z"
  }
}


‚∏ª

Sistema de cache (record / replay)

Cache key

Clave estable basada en:

sha256(
  prompt_id +
  prompt_version +
  model +
  normalized(inputs)
)

Flujo interno
	1.	Calcular hash
	2.	Buscar en cache
	3.	Si existe ‚Üí devolver respuesta (REPLAY)
	4.	Si no existe:
	‚Ä¢	cache_only ‚Üí error expl√≠cito
	‚Ä¢	mock ‚Üí generar mock
	‚Ä¢	live ‚Üí llamar al modelo y grabar cache

‚∏ª

Backends de cache posibles
	‚Ä¢	n8n Data Store (simple)
	‚Ä¢	Redis (si ya existe)
	‚Ä¢	Sistema de ficheros (recomendado)

Ejemplo:

/storage/llm_cache/
  classify_image_v3/
    <hash>.json


‚∏ª

Mocks deterministas

El modo mock debe devolver:
	‚Ä¢	JSON v√°lido seg√∫n schema
	‚Ä¢	Siempre igual para el mismo prompt_id
	‚Ä¢	√ötil para probar l√≥gica downstream

Ejemplo:

{
  "type": "whiteboard",
  "confidence": 0.92
}


‚∏ª

Prompts y fixtures versionados en Git

Estructura recomendada:

prompts/
  classify_image/
    prompt.md
    schema.json
    fixtures/
      input_01.json
      output_01.json

Ventajas:
	‚Ä¢	Control de versiones
	‚Ä¢	Revisi√≥n por PR
	‚Ä¢	Cambiar prompt_version invalida cache autom√°ticamente

‚∏ª

Force refresh (uso controlado)

Permite regenerar una respuesta aunque exista cache:

"force_refresh": true

Solo debe usarse en desarrollo.

‚∏ª

Flujo de trabajo recomendado

Desarrollo
	‚Ä¢	LLM_MODE=replay
	‚Ä¢	Iterar l√≥gica sin coste
	‚Ä¢	Cambiar a live solo para regenerar fixtures

Producci√≥n
	‚Ä¢	LLM_MODE=live
	‚Ä¢	Cache activo para coste y latencia

‚∏ª

Beneficios clave
	‚Ä¢	üö´ Cero llamadas LLM durante iteraci√≥n
	‚Ä¢	üí∏ Control total de costes
	‚Ä¢	üîÅ Reproducibilidad exacta
	‚Ä¢	üß† Separaci√≥n clara entre l√≥gica y IA
	‚Ä¢	üì¶ Arquitectura escalable para nuevos casos

‚∏ª

Pr√≥ximos pasos
	1.	Crear sub-workflow llm.call
	2.	Definir estructura prompts/ en el repo
	3.	Implementar cache en /storage
	4.	Migrar workflows existentes al adapter

‚∏ª

Estado objetivo: desarrollo r√°pido, barato y predecible, con IA tratada como una dependencia controlada y no como un efecto colateral.